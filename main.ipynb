{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF702 Redes Neurais\n",
    "Projeto de redes neurais utilizando Base de Dados do Tipo 2, Detecção de Células de Câncer em Mamografias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a leitura da base de dados foi feita utilizando a biblioteca pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('mammography.csv')\n",
    "\n",
    "data_set.columns = ['X1','X2','X3','X4','X5','X6','CLASS'] # renomeando as colunas para ficar CLASS em vez de class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo exemplos repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data_set.columns.tolist()[:-1] # remove a coluna da classe da lista de colunas\n",
    "# print (columns)\n",
    "data_set.drop_duplicates(subset=columns, # seleciona apenas as 6 primeiras colunas para verificar duplicatas\n",
    "                         keep=False, # remove todos os exemplos repetidos\n",
    "                         inplace=True)  # Remove exemplos repetidos\n",
    "print (len(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renomeando a classe -1 para 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set['CLASS'] = data_set['CLASS'].map(lambda x : 0 if (x == -1) else 1)\n",
    "# print (data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estatisticas da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas sobre as variáveis\n",
    "data_set.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando as classes da base de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando função para separando a base de dados pelas classes, para assim poder garantir que vai ter exemplos de cada classe em todos os conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separar_classes(data):\n",
    "    zero = data[data.CLASS == 0]\n",
    "    um = data[data.CLASS == 1]\n",
    "    \n",
    "    return [zero, um]\n",
    "\n",
    "# print (len(separar_grupos(data_set)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão dos Dados em Treino, Validação, e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separar_grupos_tvt(data): \n",
    "    \"\"\"\n",
    "    Divisão da base de dados\n",
    "    Treinamento = 50%\n",
    "    Validação = 25%\n",
    "    Teste = 25%\n",
    "    \"\"\"\n",
    "    \n",
    "    # classe zero\n",
    "    zero_train, zero_validation = train_test_split(data[0], # base de dados que vai ser dividida\n",
    "                                                   test_size=1/2, # proporção da divisão dos dados\n",
    "                                                   random_state=42)\n",
    "    zero_validation, zero_teste = train_test_split(zero_validation, # base de dados que vai ser dividida\n",
    "                                                   test_size=1/2, # proporção da divisão dos dados\n",
    "                                                   random_state=42)\n",
    "    \n",
    "    #classe um\n",
    "    um_train, um_validation = train_test_split(data[1], # base de dados que vai ser dividida\n",
    "                                                   test_size=1/2, # proporção da divisão dos dados\n",
    "                                                   random_state=42)\n",
    "    um_validation, um_teste = train_test_split(um_validation, # base de dados que vai ser dividida\n",
    "                                                   test_size=1/2, # proporção da divisão dos dados\n",
    "                                                   random_state=42)\n",
    "    \n",
    "    return [(zero_train, zero_validation, zero_teste),(um_train, um_validation, um_teste)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicando os dados da classe minoritaria para ter a mesma quantidade de exemplos das duas classes na MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oversampling_replacement(data):\n",
    "    um_train = data[1][0]\n",
    "    um_validation = data[1][1]\n",
    "    um_train = np.resize(um_train, data[0][0].shape)\n",
    "    um_validation = np.resize(um_validation, data[0][1].shape)\n",
    "    \n",
    "    return [data[0],(um_train, um_validation, data[1][2])]\n",
    "\n",
    "def oversampling_SMOTE(data):\n",
    "    '''Faz o oversampling usando o algoritmo SMOTE\n",
    "    \n",
    "    Parametros:\n",
    "        data (array-like): Array das amostras, com as amostras de treinamento no 1o indice, de validacao no 2o e teste no 3o\n",
    "    \n",
    "    Returns:\n",
    "        array-like: Array das amostras, apos o oversampling\n",
    "    '''\n",
    "    sm = SMOTE(random_state=42)\n",
    "    \n",
    "    train_features = data[0][:, :-1]\n",
    "    train_labels = data[0][:, -1]\n",
    "    features, labels = sm.fit_sample(train_features, train_labels)\n",
    "    train = np.zeros((len(labels), 7))\n",
    "    for i in range(len(train)):\n",
    "        train[i] = np.concatenate((features[i], np.array([labels[i]])), axis=0)\n",
    "    # Sem isso, os 0s tenderiam a ficar acima dos 1s\n",
    "    np.random.shuffle(train)\n",
    "    \n",
    "    validation_features = data[1][:, :-1]\n",
    "    validation_labels = data[1][:, -1]\n",
    "    features, labels = sm.fit_sample(validation_features, validation_labels)\n",
    "    validation = np.zeros((len(labels), 7))\n",
    "    for i in range(len(validation)):\n",
    "        validation[i] = np.concatenate((features[i], np.array([labels[i]])), axis=0)\n",
    "    np.random.shuffle(validation)\n",
    "    \n",
    "    return [train, validation, data[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntando as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntando as classes zero e um dos conjuntos de treinamento, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_class(data):\n",
    "    train = np.concatenate((data[0][0], data[1][0]), axis=0)\n",
    "    validation = np.concatenate((data[0][1], data[1][1]), axis=0)\n",
    "    test = np.concatenate((data[0][2], data[1][2]), axis=0)\n",
    "\n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(validation)\n",
    "    np.random.shuffle(test)\n",
    "    \n",
    "    return [train, validation, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_oversampling = oversampling(separar_grupos_tvt(separar_classes(data_set)))\n",
    "# print (data_set_oversampling)\n",
    "sep = separar_classes(data_set)\n",
    "print (len(sep[0]), len(sep[1]))\n",
    "grupos = separar_grupos_tvt(sep)\n",
    "print (len(grupos[0][0]), len(grupos[0][1]), len(grupos[0][2]), \n",
    "       len(grupos[1][0]), len(grupos[1][1]), len(grupos[1][2]))\n",
    "#over = oversampling_replacement(grupos)\n",
    "#print (len(over[0][0]), len(over[0][1]), len(over[0][2]), len(over[1][0]), len(over[1][1]), len(over[1][2]))\n",
    "\n",
    "#join_c = join_class(over)\n",
    "#print(len(join_c[0]), len(join_c[1]), len(join_c[2]))\n",
    "#print(len(join_c[0][0]))\n",
    "#print(join_c[0][0])\n",
    "\n",
    "join_c = join_class(grupos)\n",
    "print (len(join_c[0]), len(join_c[1]), len(join_c[2]))\n",
    "\n",
    "over = oversampling_SMOTE(join_c)\n",
    "print (len(over[0]), len(over[1]), len(over[2]))\n",
    "print(len(over[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando entrada de saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = data_set.columns.tolist()\n",
    "# shape_view = data_set.reindex(columns=columns[1:10] + [columns[0]]) # columns[0] é a coluna que fica qual é a classe que o exemplo pertence\n",
    "# rgb_view = data_set.reindex(columns=columns[10:] + [columns[0]]) # columns[0] é a coluna que fica qual é a classe que o exemplo pertence\n",
    "X_train = join_c[0][:,:-1]\n",
    "y_train = join_c[0][:,-1]\n",
    "\n",
    "X_validation = join_c[1][:,:-1]\n",
    "y_validation = join_c[1][:,-1]\n",
    "\n",
    "X_test = join_c[2][:,:-1]\n",
    "y_test = join_c[2][:,-1]\n",
    "# print (y_validation)\n",
    "\n",
    "\n",
    "# utilizado para verificar a quantidade de exemplos de cada classe que tem nos conjuntos de validação, teste e treinamento\n",
    "import collections\n",
    "print (collections.Counter(y_train))\n",
    "print (collections.Counter(y_validation))\n",
    "print (collections.Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição e Treino da Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas funções auxiliares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_final_losses(history):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    idx_min_val_loss = np.argmin(val_loss)\n",
    "    \n",
    "    return {'train_loss': train_loss[idx_min_val_loss], 'val_loss': val_loss[idx_min_val_loss]}\n",
    "\n",
    "def plot_training_error_curves(history):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.set(title='Training and Validation Error Curves', xlabel='Epochs', ylabel='Loss (MSE)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_training_acc_curves(history):\n",
    "    train_loss = history.history['acc']\n",
    "    val_loss = history.history['val_acc']\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.set(title='Training and Validation Accuracy Curves', xlabel='Epochs', ylabel='Accuracy')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA 1 (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(3, activation='relu', input_dim=6)) # camada escondida\n",
    "classifier.add(Dense(1, activation='relu')) # \n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='mean_squared_error', # metrica de erro\n",
    "                   metrics=['accuracy']) # metrica de sucesso\n",
    "\n",
    "history = classifier.fit(X_train, y_train,\n",
    "                         epochs=100, # quantidade de epocas que a rede neural vai executar\n",
    "                         verbose=0,\n",
    "                         shuffle=True, # utilizado para misturar as amostras a cada epoca\n",
    "                         validation_data=(X_validation, y_validation))\n",
    "#                          callbacks=[early_stopping], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(X_test, y_test)\n",
    "# print (test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_training_error_curves(history)\n",
    "plot_training_acc_curves(history)\n",
    "# plot_training_roc_curves(y_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA 2 (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(5, activation='relu', input_dim=6)) # camada escondida\n",
    "classifier.add(Dense(1, activation='relu')) # \n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='mean_squared_error', # metrica de erro\n",
    "                   metrics=['accuracy']) # metrica de sucesso\n",
    "\n",
    "history = classifier.fit(X_train, y_train,\n",
    "                         epochs=100, # quantidade de epocas que a rede neural vai executar\n",
    "                         verbose=0,\n",
    "                         shuffle=True, # utilizado para misturar as amostras a cada epoca\n",
    "                         validation_data=(X_validation, y_validation))\n",
    "#                          callbacks=[early_stopping], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = classifier.evaluate(X_test, y_test)\n",
    "print (test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training_error_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA 3 (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(3, activation='relu', input_dim=6)) # camada escondida\n",
    "classifier.add(Dense(5, activation='relu')) # camada escondida\n",
    "classifier.add(Dense(1, activation='relu')) # \n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='mean_squared_error', # metrica de erro\n",
    "                   metrics=['accuracy']) # metrica de sucesso\n",
    "\n",
    "history = classifier.fit(X_train, y_train,\n",
    "                         epochs=100, # quantidade de epocas que a rede neural vai executar\n",
    "                         verbose=0,\n",
    "                         shuffle=True, # utilizado para misturar as amostras a cada epoca\n",
    "                         validation_data=(X_validation, y_validation))\n",
    "#                          callbacks=[early_stopping], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = classifier.evaluate(X_test, y_test)\n",
    "print (test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_error_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA 4 (5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(5, activation='relu', input_dim=6)) # camada escondida\n",
    "classifier.add(Dense(3, activation='relu')) # camada escondida\n",
    "classifier.add(Dense(1, activation='relu')) # \n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='mean_squared_error', # metrica de erro\n",
    "                   metrics=['accuracy']) # metrica de sucesso\n",
    "\n",
    "history = classifier.fit(X_train, y_train,\n",
    "                         epochs=100, # quantidade de epocas que a rede neural vai executar\n",
    "                         verbose=0,\n",
    "                         shuffle=True, # utilizado para misturar as amostras a cada epoca\n",
    "                         validation_data=(X_validation, y_validation))\n",
    "#                          callbacks=[early_stopping], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = classifier.evaluate(X_test, y_test)\n",
    "print (test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_error_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA 5 (4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(4, activation='relu', input_dim=6)) # camada escondida\n",
    "classifier.add(Dense(2, activation='relu')) # camada escondida\n",
    "classifier.add(Dense(1, activation='relu')) # \n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='mean_squared_error', # metrica de erro\n",
    "                   metrics=['accuracy']) # metrica de sucesso\n",
    "\n",
    "history = classifier.fit(X_train, y_train,\n",
    "                         epochs=100, # quantidade de epocas que a rede neural vai executar\n",
    "                         verbose=0,\n",
    "                         shuffle=True, # utilizado para misturar as amostras a cada epoca\n",
    "                         validation_data=(X_validation, y_validation))\n",
    "#                          callbacks=[early_stopping], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = classifier.evaluate(X_test, y_test)\n",
    "print (test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_error_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predições no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora utilizamos a nossa rede para fazer predições no conjunto de teste e computar métricas de desempenho.\n",
    "\n",
    "Além das métricas utilizadas aqui, mais métricas de desempenho podem ser encontradas em: http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fazer predições no conjunto de teste\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_class = classifier.predict_classes(X_test, verbose=0)\n",
    "\n",
    "## Matriz de confusão\n",
    "print('Matriz de confusão')\n",
    "print(confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "## Computar métricas de desempenho\n",
    "losses = extract_final_losses(history)\n",
    "print()\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Train Loss:\", value=losses['train_loss']))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Validation Loss:\", value=losses['val_loss']))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redesNeurais",
   "language": "python",
   "name": "redesneurais"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
